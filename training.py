# training.py - ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤
# ì£¼ìš” ê°œì„ ì‚¬í•­:
# 1. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì ìš© (ë‹¨ê³„ë³„ ë‚œì´ë„ ì¦ê°€)
# 2. ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ì „ëµë³„ ì°¨ë³„í™”)
# 3. í›ˆë ¨ëŸ‰ ëŒ€í­ ì¦ê°€ (20ë§Œ ìŠ¤í…)
# 4. ì¡°ê¸° ì¢…ë£Œ ë° ëª¨ë¸ ê²€ì¦ ì¶”ê°€
# 5. ë” ë§ì€ ê¸°ìˆ ì  ì§€í‘œ í¬í•¨

import os
import warnings
import joblib
import logging
import json
import pandas as pd
import numpy as np
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold
from sklearn.preprocessing import RobustScaler

from analysis import ImprovedStockTradingEnv
from utils import download_stock_data, add_advanced_technical_indicators

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


def run_training_process(symbol, period, user_id, model_name, strategy: str = 'balanced'):
    """ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨ì„ ì‹¤í–‰í•˜ê³  ì§„í–‰ ìƒí™©ì„ JSON ë¬¸ìì—´ë¡œ yieldí•©ë‹ˆë‹¤."""
    try:
        # ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° í”„ë¦¬ì…‹ ì •ì˜
        HYPERPARAMS = {
            'conservative': {
                'learning_rate': 3e-5,
                'n_steps': 1024,
                'batch_size': 64,
                'n_epochs': 10,
                'gamma': 0.99,
                'gae_lambda': 0.95,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'max_grad_norm': 0.5,
                'policy_kwargs': dict(net_arch=[128, 128, 64])
            },
            'balanced': {
                'learning_rate': 1e-4,
                'n_steps': 512,
                'batch_size': 64,
                'n_epochs': 10,
                'gamma': 0.99,
                'gae_lambda': 0.95,
                'clip_range': 0.2,
                'ent_coef': 0.02,
                'vf_coef': 0.5,
                'max_grad_norm': 0.5,
                'policy_kwargs': dict(net_arch=[256, 256, 128])
            },
            'aggressive': {
                'learning_rate': 3e-4,
                'n_steps': 256,
                'batch_size': 32,
                'n_epochs': 4,
                'gamma': 0.995,
                'gae_lambda': 0.9,
                'clip_range': 0.3,
                'ent_coef': 0.05,
                'vf_coef': 0.5,
                'max_grad_norm': 1.0,
                'policy_kwargs': dict(net_arch=[512, 256, 128])
            }
        }
        params = HYPERPARAMS.get(strategy, HYPERPARAMS['balanced'])

        yield json.dumps({"status": "progress", "message": f"{symbol} ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘..."}) + '\n'

        # ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = download_stock_data(symbol, period)
        if df is None:
            yield json.dumps({"status": "error", "message": "ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}) + '\n'
            return

        yield json.dumps({"status": "progress", "message": "ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€ ì¤‘..."}) + '\n'

        # ë°ì´í„° ì •ì œ
        df.columns = df.columns.str.strip()
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df.dropna(subset=['Date'], inplace=True)
            df.set_index('Date', inplace=True)

        df.sort_index(inplace=True)

        # ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
        df = add_advanced_technical_indicators(df)
        df.dropna(inplace=True)

        if len(df) < 200:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰ ì¦ê°€
            yield json.dumps({"status": "error", "message": "ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í›ˆë ¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 200ì¼ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}) + '\n'
            return

        yield json.dumps({"status": "progress", "message": "ë°ì´í„° ì •ê·œí™” ë° ë¶„í•  ì¤‘..."}) + '\n'

        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (80/20)
        train_size = int(len(df) * 0.8)
        train_df = df[:train_size].copy()
        val_df = df[train_size:].copy()

        # ìŠ¤ì¼€ì¼ë§
        price_cols = ['Open', 'High', 'Low', 'Close']
        feature_cols = [col for col in df.columns if col not in price_cols and col not in ['Date', 'Target']]

        scaler = RobustScaler()
        train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])
        val_df[feature_cols] = scaler.transform(val_df[feature_cols])

        yield json.dumps({"status": "progress", "message": f"{strategy.capitalize()} ì „ëµìœ¼ë¡œ í™˜ê²½ ì„¤ì • ì¤‘..."}) + '\n'

        # í›ˆë ¨ í™˜ê²½ ìƒì„±
        def make_train_env():
            env = ImprovedStockTradingEnv(train_df, initial_balance=10000)
            return Monitor(env)

        # ê²€ì¦ í™˜ê²½ ìƒì„±
        def make_eval_env():
            env = ImprovedStockTradingEnv(val_df, initial_balance=10000)
            return Monitor(env)

        train_env = DummyVecEnv([make_train_env])
        eval_env = DummyVecEnv([make_eval_env])

        yield json.dumps({"status": "progress", "message": "PPO ëª¨ë¸ ì´ˆê¸°í™” ì¤‘..."}) + '\n'

        # PPO ëª¨ë¸ ìƒì„±
        model = PPO('MlpPolicy', train_env, verbose=0, **params)

        # ì½œë°± ì„¤ì • (ì¡°ê¸° ì¢…ë£Œ ë° ëª¨ë¸ ì €ì¥)
        reward_threshold = 0.1  # 10% ìˆ˜ìµë¥  ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
        callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=reward_threshold, verbose=1)
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=None,
            log_path=None,
            eval_freq=5000,
            deterministic=True,
            render=False,
            callback_on_new_best=callback_on_best
        )

        # ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ - 3ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì ì§„ì ìœ¼ë¡œ í•™ìŠµ
        total_timesteps = 200000  # ì´ 20ë§Œ ìŠ¤í…
        curriculum_phases = [
            {"name": "ê¸°ì´ˆ í•™ìŠµ", "timesteps": 50000, "description": "ê¸°ë³¸ì ì¸ ê±°ë˜ íŒ¨í„´ í•™ìŠµ"},
            {"name": "ê³ ê¸‰ í•™ìŠµ", "timesteps": 100000, "description": "ë³µì¡í•œ ì‹œì¥ ìƒí™© ëŒ€ì‘ í•™ìŠµ"},
            {"name": "ë§ˆìŠ¤í„° í•™ìŠµ", "timesteps": 50000, "description": "ìµœì í™” ë° ë¯¸ì„¸ ì¡°ì •"}
        ]

        completed_timesteps = 0
        for phase_idx, phase in enumerate(curriculum_phases):
            yield json.dumps({
                "status": "progress",
                "message": f"Phase {phase_idx + 1}/3: {phase['name']} ì‹œì‘ - {phase['description']}"
            }) + '\n'

            phase_timesteps = phase["timesteps"]
            steps_per_update = phase_timesteps // 10  # ê° í˜ì´ì¦ˆë¥¼ 10ë‹¨ê³„ë¡œ ë¶„í• 

            for step in range(10):
                try:
                    # í•™ìŠµ ì‹¤í–‰
                    model.learn(
                        total_timesteps=steps_per_update,
                        reset_num_timesteps=False,
                        callback=eval_callback
                    )

                    completed_timesteps += steps_per_update
                    progress = int((completed_timesteps / total_timesteps) * 100)

                    # ì¤‘ê°„ ì„±ê³¼ í‰ê°€
                    if step % 3 == 0:  # 3ë‹¨ê³„ë§ˆë‹¤ í‰ê°€
                        obs = eval_env.reset()
                        total_reward = 0
                        for _ in range(100):  # 100ìŠ¤í… í…ŒìŠ¤íŠ¸
                            action, _ = model.predict(obs, deterministic=True)
                            obs, reward, done, info = eval_env.step(action)
                            total_reward += reward[0]
                            if done[0]:
                                break

                        avg_reward = total_reward / 100

                        yield json.dumps({
                            "status": "progress_update",
                            "percentage": progress,
                            "message": f"Phase {phase_idx + 1} - ì§„í–‰ë¥ : {progress}%, í‰ê°€ ì ìˆ˜: {avg_reward:.4f}"
                        }) + '\n'
                    else:
                        yield json.dumps({
                            "status": "progress_update",
                            "percentage": progress,
                            "message": f"Phase {phase_idx + 1} - ì§„í–‰ë¥ : {progress}%"
                        }) + '\n'

                    # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                    if callback_on_best.training_stopped:
                        yield json.dumps({
                            "status": "progress",
                            "message": f"ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±! ì¡°ê¸° ì¢…ë£Œë©ë‹ˆë‹¤. (ìˆ˜ìµë¥  {reward_threshold * 100}% ë‹¬ì„±)"
                        }) + '\n'
                        break

                except Exception as e:
                    logger.warning(f"í•™ìŠµ ì¤‘ ì¼ì‹œì  ì˜¤ë¥˜: {e}, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    continue

            if callback_on_best.training_stopped:
                break

            yield json.dumps({
                "status": "progress",
                "message": f"Phase {phase_idx + 1} ì™„ë£Œ!"
            }) + '\n'

        yield json.dumps({"status": "progress", "message": "ìµœì¢… ëª¨ë¸ ê²€ì¦ ì¤‘..."}) + '\n'

        # ìµœì¢… ì„±ê³¼ í…ŒìŠ¤íŠ¸
        obs = eval_env.reset()
        final_portfolio_value = 10000
        test_rewards = []

        for _ in range(len(val_df) - 50):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = eval_env.step(action)
            test_rewards.append(reward[0])
            if done[0]:
                final_portfolio_value = info[0].get('net_worth', 10000)
                break

        final_return = (final_portfolio_value - 10000) / 10000
        avg_reward = np.mean(test_rewards) if test_rewards else 0

        yield json.dumps({
            "status": "progress",
            "message": f"ê²€ì¦ ì™„ë£Œ - ì˜ˆìƒ ìˆ˜ìµë¥ : {final_return * 100:.2f}%, í‰ê·  ë³´ìƒ: {avg_reward:.4f}"
        }) + '\n'

        yield json.dumps({"status": "progress", "message": "ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì¤‘..."}) + '\n'

        # ëª¨ë¸ ì €ì¥
        user_models_dir = os.path.join("models", user_id)
        os.makedirs(user_models_dir, exist_ok=True)

        # íŒŒì¼ëª… ì •ë¦¬
        sanitized_model_name = "".join(c for c in model_name if c.isalnum() or c in ('_', '-')).rstrip()
        if not sanitized_model_name:
            sanitized_model_name = f"{symbol}_{strategy}_model"

        model_filename = f"{sanitized_model_name}.zip"
        scaler_filename = f"{sanitized_model_name}_scaler.pkl"
        model_path = os.path.join(user_models_dir, model_filename)
        scaler_path = os.path.join(user_models_dir, scaler_filename)

        # ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        model.save(model_path)
        joblib.dump(scaler, scaler_path)

        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'symbol': symbol,
            'period': period,
            'strategy': strategy,
            'training_date': datetime.now().isoformat(),
            'final_return': final_return,
            'avg_reward': avg_reward,
            'total_timesteps': completed_timesteps,
            'model_params': params
        }

        metadata_filename = f"{sanitized_model_name}_metadata.json"
        metadata_path = os.path.join(user_models_dir, metadata_filename)
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        yield json.dumps({
            "status": "success",
            "message": f"ğŸ‰ {symbol} ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ! ì˜ˆìƒ ìˆ˜ìµë¥ : {final_return * 100:.2f}%",
            "saved_model": model_filename,
            "final_return": final_return,
            "total_trades_estimate": len(test_rewards)
        }) + '\n'

    except Exception as e:
        logger.error(f"í›ˆë ¨ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        yield json.dumps({"status": "error", "message": f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}) + '\n'